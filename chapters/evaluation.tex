%% ==============
\chapter{Evaluation}
\label{ch:Evaluation}
%% ==============
In this chapter we compare all the techniques we implemented with plain uniform and power-based Next Event Estimation and PBRTs implementation. We try to point out the strength and weaknesses of the techniques for certain kind of scenes and scenarios. In this context we mainly argue about image quality, time and memory. We compare time and memory consumption based on the Root Mean Squared Error (RMSE) metric from equation~\ref{eq:rmse}. Where $Y_i$ is the vector of pixels of length $n$ of our reference image and $\widetilde{Y}_i$ is the vector of pixels of the compared image. Note that for unbiased techniques the variance $\sigma^2$ is equal to the MSE, hence we use both interchangeably in various contexts. 

\begin{align}\label{eq:mse}
\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}\abs{Y_i - \widetilde{Y}_i}^2
\end{align}
\begin{align}\label{eq:rmse}
\text{RMSE} = \sqrt{\text{MSE}}
\end{align}


We often plot and compare based on RMSE, therefor the RMSE is an estimator for our image quality. When we argue about image quality we rather refer to the perceived image quality\footnote{The RMSE in itself has a perception component because of the square. The RMSE is a very commonly used estimator, nonetheless one could imagine using a metric which does reflect human perception more closely, see for example the structural similarity index (SSIM) \parencite{DBLP:journals/tip/WangBSS04}. This usually leads to problems with subjectivity.}, in our case this usually is determined by whether sensible artifacts are present. As all the compared techniques are unbiased, given unlimited time, all of them would converge to the same image. Artifacts are usually areas or edges which will converge very slowly, in practice maybe never. When we say a technique has a bad image quality, we mean that artifacts are present. 

\begin{align}\label{eq:sigmaCor}
\sigma \sim \frac{1}{\sqrt{N}}
\end{align}

...

\begin{figure}
\centering
\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=1\linewidth]{figures/examples/StanfordMuseum_pvox_ps512_t503_icdf-0_pc96000k_mc0,1_Vox96_8854.png}
   \caption{Stanford-Museum rendered with 512 samples per pixel with \textit{cdfgrid} and no interpolation.}
   \label{fig:SMnoInt} 
\end{subfigure}

\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=1\linewidth]{figures/examples/StanfordMuseum_pvox_ps512_t723_icdf-1_pc128000k_mc0,1_Vox64_17574.png}
   \caption{Stanford-Museum rendered with 512 samples per pixel with \textit{cdfgrid} and trilinear interpolation.}
   \label{fig:SMInt}
\end{subfigure}

\caption{A visual comparison of no interpolation vs trilinear interpolation. Even though we already have a rather high sample per pixel count, (a) shows many clear variance edges of the underlying grid cells and various artifacts across the scene. Also it is remarkable how many fireflies are visible and how almost none of them are present in (b). It is quite apparent that a human observer would grade the image quality of (b) much higher, but surprisingly the RMSE of (a) is significantly smaller, $39.8$ versus $47.3$. The reason being, that aside of the fireflies and artifacts, the majority of pixels in (a) are less noisy. The difference in noise is less apparent at first, but is well illustrated on the white/gray wall to the left of the Buddah.}
\end{figure}


\section{Setup}

The test machine we use for comparisons is an Intel i7-4790K CPU @ 4GHz, 32 GB RAM, running on Windows 10 64-Bit. Images are produced by PBRT-v3, forked on March 30\footnote{Latest commit before the fork: \\ \url{https://github.com/mmp/pbrt-v3/commit/42c42c194bab970d8adc3f6b5e3afbbc172c3375}}. PNEE techniques are added on top of this fork, the complete implementation can be found at \url{https://github.com/AndiMiko/pbrt-v3}.

\subsection{Problem cases}

We tried to identify problematic light and object constellations, which are causing trouble for various kinds of techniques. The most prominent and comprehensible scenarios are to be covered bellow. 

\begin{description}
    \item[1. High Frequency.] Rapid changes of close pixels can happen for two main reasons. Either there is a physical edge and thus the rays from the eye hit another object, or an object cast a hard shadow. The human observer is more forgiving when the variance changes together with the object material, while variance changes for the latter scenario usually are more irritating. With a lot fine shadows respectively strong light sources casting sharp light edges choosing the correct light source to sample can get tricky. A powerful light source can dominate an estimating data structure but may only be visible in a few spots. An estimator thus can quickly become bad while it was perfectly fine for a neighboring pixel. 
    \item[2. Level of Detail.] Highly varying object sizes and as such potential detail is the cause for a set of well known problems in computer graphics. Especially, the choice of data structures are affected. A data structure can store a lot of data and thus spend a lot of resources on some big object but in the end the camera might only focus on a different tiny detail and as such make most of the data useless. This problems are usually addressed with adaptive data structures.
    \item[3. High variantion of light power.] Some algorithms rely on a correlation between incident flux on a shading point and the power of a light source. This is the basis for the power-based technique compared to uniform. Also the number of photons shot from a light source can be based on the light power and thus its alleged importance. This assumption is often useful, but can quickly cause problems when a strong light source is occluded close to a small light source which illuminates important details.
    \item[4. High number of contributing light sources.] The construction and lookup times of the acceleration data structures have to scale well, so that even in the case that a shading point is illuminated by many light sources rendering can stay efficient.
    \item[5. Non-axis aligned planes.] Skewed planes usually won't alight with the boundaries of the underlying data structures unless data structures like a BSP tree are used. Especially, rigid data structures like a grid can produce artifacts when the estimator seemingly arbitrarily changes. 
    \item[6. Highly occluding areas.] Large dark areas where most parts are occluded except a few spots where just a handful of photons may arrive can cause problems when trying to build meaningful estimators. Adaptive techniques may increase their search radius which can lead to unexpected behavior. Or estimators can't be build meaningfully with too few photons available.
    \item[7. Tiny but important solid angles.] The difference between a black box and an illuminated box can be a tiny hole or crack where a lot of light may enter and then scatter within. Also a small object that gets illuminated by a very strong but distant light source, e.g. the sun. The solid angle might by so tiny, that it is numerically very unlikely that photons would ever hit the object, but it may still be the main source of brightness.
\label{li:problemcases}
\end{description} 

\subsection{Test scenes}

We designed a special scene, called \textit{Stanford-Museum}, for most of the comparisons we present in this chapter. This scene covers all aforementioned problem cases and thus gives a good impression about strengths and weaknesses of each technique. The simplicity of the scene allows the reader to clearly correlate variance with the problem cases, but ultimately does not provide a photorealistic appeal. On the other hand we present two more scenes, \textit{San Miguel} and \textit{Zero-day}, for a comparison on real scenes with all sorts of details.

\paragraph{Stanford-Museum}

\begin{figure}[ht]\centering
\input{figures/StanfordMuseum.tex}
\caption{The reference image for \textit{Stanford-Museum} sampled with PBRTs directlighting integrator. Instead of NEE every light source is queried for any shading point. Max depth is one, thus no indirect illumination is present. All phenomenons from listing~\ref{li:problemcases} and more appear in this scene. Detailed descriptions and intentions of the zoomed areas are listed in~\ref{li:stanfordmuseum}.
}
\label{fig:stanfordmuseumref}
\end{figure}

We describe the most important problem cases we had in mind when designing the Stanford-Museum scene in figure~\ref{fig:stanfordmuseumref}, which is the reference image we calculate the RMSE against. The scene contains 4058 light sources, all of whom are point light sources. We chose point light sources, because sampling a point on a area light source is not a concern of PNEE, thus reducing variance from other effects does disclose artifacts produced by our techniques more clearly. We also render the scene with a pathtracer with a max depth parameter of one, again, using the same argument, as depth introduces variance from the indirect lighting term of the pathtracer. Additionally, this makes rendering a nearly perfect reference image possible, by using PBRTs direct lighting integrator, which samples all point light sources for any intersection point. The reference image is rendered with 128 samples per pixel in 17 hours on our test machine and aside from anti-aliasing should be close to perfect. 

\begin{itemize}
    \item[(1)] The dragon is 20 times smaller than the Buddah but is very close to the camera and as such takes up almost the same screen space. Several low intensity point lights illuminate the dragon. The difference in size as well as light intensity constitute typical problem cases with LOD. Especially the illumination of the eyes is a rather extreme case, light source intensity is roughly 200.000 times less compared to the strongest light sources in the scene. Additionally, the left outline of the dragon is illuminated by various light sources from bellow in the scene, where the dragon takes up only an incredibly small solid angle and thus photons are very unlikely to hit the dragon.
    \item[(2)] The Buddah stands out of all cells and as such is illuminated by almost all visible light sources in this scene. This is particularly a problem for example with \textit{photontree}. There might be more light sources influencing a shading point, than the total number of nearest neighbors we collect. High geometric detail and blending of many light colors make the Buddah prone to fireflies.  All three Stanford models are also intended to add complexity for intersection tests to various parts of the scene. 
    \item[(3)] The box has a very tiny split from which is casts long range, very bright, very thin and high frequent light rays. Additionally, a lot of photons are stuck within the thin walled box, which highly complicates building good estimators with PNEE on the outside. Several more of this boxes with varying properties and surroundings are present in the scene. 
    \item[(4)] Similar to the Buddah this plane wall is illuminated by many light sources in the scene. Additionally, there is a matrix of point lights with varying brightness, distance and color. This variety has to be mapped to smooth transitions and high frequencies accordingly. Similar scenarios to this one do also appear in other parts of the scene, for example in front of the bunny, also with many smooth transitions, but where difficulties brought by global illumination are replaced with many local soft shadows. 
    \item[(5)] A cluster of roughly 1000 light sources. Only thin, non axis-aligned walls separates it from several distinct lightning scenarios. Very high exposure (unintentionally) breaks anti-aliasing. Most light sources influence the Buddah softly from bellow. 
    \item[(6)] Several non-axis aligned thin walls cover up a small cluster of light sources and cast thin, high frequent light rays in many directions. Especially the rays casted onto the scewed box, where photons from within the box, like mentioned in (3), do no good for our estimators, do constitute a tricky shading scenario in various manners.
    \item[(7)] Several strong light sources cast long range, smoothly fading, but sharp edged light rays. A similar scenario is present on the other side of the Buddah, where colors also blend together. The rays closely pass quite dark cells. Again, strong light sources behind a thin wall interfere with our photon collection algorithms.
    \item[(8)] A completely occluded light cluster with roughly 1000 light sources. In a real scene this might be a different room of a house for example. There are four such occluded clusters in the scene.
\label{li:stanfordmuseum}
\end{itemize}
\paragraph{San Miguel}
\textcite{Sanmiguel}



\paragraph{Zero-Day}

\parencite{Beeple}




\subsection{Techniques}

We give an brief overview over the techniques that we identified as interesting for comparison. \textit{Uniform} and \textit{power-based} sampling are naive techniques that are natively implemented in PBRT. Additionally, PBRTs just recently released take on many light sampling is referred to as \textit{spatial}, see section~\ref{}\todo{add reference to previouswork when done}. We compare these three existing techniques with our own techniques \textit{cdfgrid}, \textit{photontree} and \textit{cdftree} introduced in Table~\ref{tb:techniques}. All three of our techniques use interpolation, as this produces the cleanest results for human observers. Nonetheless, we also list \textit{cdfgrid no Int}, which uses no interpolation. The images produced by this technique usually contain artifacts, but nonetheless performs very well on the MSE plots as the overall variance is better. For later plots we spare previously clearly worse performing techniques to focus our discussion.

\subsection{Parameter Comparison}



%%%%%%%%%%%%%%%%%
\label{ch:ev:photontree}

%%%%%%%%%%%%%%%%%
\label{ch:ev:cdftree}

%%%%%%%%%%%%%%%%%%%%

\label{ch:ev:photonsampling}


%%%%%%%%%%%%%%%%%%%%%%%
\label{ch:ev:uniformfloor}

% which parameters did we try to configure. Which parameters turned out to be good and will be set as fixed?

\section{Equal time comparisons}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/plots/SM_all_mse_report.pdf}
    \caption{Stanford Museum comparison}
    \label{fig:gasfdgds}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/plots/SM_Main.pdf}
    \caption{Stanford Museum comparison}
    \label{fig:asfsgfa}
\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=1\textwidth, trim=0 0 2200 0, clip]{figures/plots/SM_Main_quad.pdf}
%    \caption{Stanford Museum comparison}
%    \label{fig:asasfffa}
%\end{figure}

\begin{figure}
    \centering
    \adjincludegraphics[width=0.5\textwidth, trim={{.0\width} {.0\height} {.25\width} {.0\height}},clip]{figures/plots/SM_Main_quad.pdf}
    \caption{Stanford Museum comparison}
    \label{fig:asddhrgfdgffa}
\end{figure}

\begin{figure}
    \centering
    \adjincludegraphics[width=1\textwidth, trim={{.0\width} {.0\height} {.25\width} {.0\height}},clip]{figures/plots/SM_Main_quad.pdf}
    \caption{Stanford Museum comparison}
    \label{fig:asghgffasfdgffa}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
      \centering
        \adjincludegraphics[width=1\textwidth, trim={{.0\width} {.0\height} {.25\width} {.0\height}},clip]{figures/plots/SM_Main_quad.pdf}
        \caption{Stanford Museum comparison}
        \label{fig:asgffagsgsgsfdgffa}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \adjincludegraphics[width=1\textwidth, trim={{.0\width} {.0\height} {.25\width} {.0\height}},clip]{figures/plots/SM_Main_quad.pdf}
        \caption{Stanford Museum comparison}
        \label{fig:asgffasgdgdssfdgffa}
    \end{subfigure}
\end{figure}

\section{Memory comparisons}

\section{Conclusions}
