%% ==============
\chapter{Evaluation}
\label{ch:Evaluation}
%% ==============
In this chapter we compare all the techniques we implemented with plain uniform and power-based Next Event Estimation and PBRTs implementation. We try to point out the strength and weaknesses of the techniques for certain kind of scenes and scenarios. In this context we mainly argue about image quality, time and memory. We compare time and memory consumption based on the Root Mean Squared Error (RMSE) metric from equation~\ref{eq:rmse}. Where $Y_i$ is the vector of pixels of length $n$ of our reference image and $\widetilde{Y}_i$ is the vector of pixels of the compared image. Note that for unbiased techniques the variance $\sigma^2$ is equal to the MSE, hence we use both interchangeably in various contexts. 

\begin{align}\label{eq:mse}
\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}\abs{Y_i - \widetilde{Y}_i}^2
\end{align}
\begin{align}\label{eq:rmse}
\text{RMSE} = \sqrt{\text{MSE}}
\end{align}


We often plot and compare based on RMSE, therefor the RMSE is an estimator for our image quality. When we argue about image quality we rather refer to the perceived image quality\footnote{The RMSE in itself has a perception component because of the square. The RMSE is a very commonly used estimator, nonetheless one could imagine using a metric which does reflect human perception more closely, see for example the structural similarity index (SSIM) \parencite{DBLP:journals/tip/WangBSS04}. This usually leads to problems with subjectivity.}, in our case this usually is determined by whether sensible artifacts are present. Figure~\ref{fig:intComparison} illustrates how the MSE and subjective image quality can differ. As all the compared techniques are unbiased, given unlimited time, all of them would converge to the same image. Artifacts are usually areas or edges which will converge very slowly, in practice maybe never. When we say a technique has a bad image quality, we mean that artifacts are present. 

As the standard deviation is proportional to the inverse root of the number of pixel samples (equation~\ref{eq:sigmaCor}) it quickly becomes unfeasible to converge an image by further increasing $N$. For this reason increasing the efficiency per pixel sample is crucial.

\begin{align}\label{eq:sigmaCor}
\sigma \sim \frac{1}{\sqrt{N}}
\end{align}

We optimized our analysis to the 128 to 2048 samples per pixel range\unsure{correct? Whats the sweetspot?}, as this is a common range for real world production. After an introduction into our setup and general test environment in the next section~\ref{sec:setup}, we continue into the most interesting part: equal time comparisons in section~\ref{sec:etc}.


\begin{figure}
\centering
\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=1\linewidth]{figures/examples/StanfordMuseum_pvox_ps512_t503_icdf-0_pc96000k_mc0,1_Vox96_8854.png}
   \caption{Stanford-Museum rendered with 512 samples per pixel with \textit{cdfgrid} and no interpolation.}
   \label{fig:SMnoInt} 
\end{subfigure}

\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=1\linewidth]{figures/examples/StanfordMuseum_pvox_ps512_t723_icdf-1_pc128000k_mc0,1_Vox64_17574.png}
   \caption{Stanford-Museum rendered with 512 samples per pixel with \textit{cdfgrid} and trilinear interpolation.}
   \label{fig:SMInt}
\end{subfigure}

\caption{A visual comparison of no interpolation vs trilinear interpolation. Even though we already have a rather high sample per pixel count, (a) shows many clear variance edges of the underlying grid cells and various artifacts across the scene. Also it is remarkable how many fireflies are visible and how almost none of them are present in (b). It is quite apparent that a human observer would grade the image quality of (b) much higher, but surprisingly the RMSE of (a) is significantly smaller, $39.8$ versus $47.3$. The reason being, that aside of the fireflies and artifacts, the majority of pixels in (a) are less noisy. The difference in noise is less apparent at first, but is well illustrated on the white/gray wall to the left of the Buddah.}
\label{fig:intComparison}
\end{figure}


\section{Setup}

The test machine we use for comparisons is an Intel i7-4790K CPU @ 4GHz, 32 GB RAM, running on Windows 10 64-Bit. Images are produced by PBRT-v3, forked on March 30\footnote{Latest commit before the fork: \\ \url{https://github.com/mmp/pbrt-v3/commit/42c42c194bab970d8adc3f6b5e3afbbc172c3375}}. PNEE techniques are added on top of this fork, the complete implementation can be found at \url{https://github.com/AndiMiko/pbrt-v3}.

\subsection{Problem cases}

We tried to identify problematic light and object constellations, which are causing trouble for various kinds of techniques. The most prominent and comprehensible scenarios are to be covered bellow. 

\begin{description}
    \item[1. High Frequency.] Rapid changes of close pixels can happen for two main reasons. Either there is a physical edge and thus the rays from the eye hit another object, or an object cast a hard shadow. The human observer is more forgiving when the variance changes together with the object material, while variance changes for the latter scenario usually are more irritating. With a lot fine shadows respectively strong light sources casting sharp light edges choosing the correct light source to sample can get tricky. A powerful light source can dominate an estimating data structure but may only be visible in a few spots. An estimator thus can quickly become bad while it was perfectly fine for a neighboring pixel. 
    \item[2. Level of Detail.] Highly varying object sizes and as such potential detail is the cause for a set of well known problems in computer graphics. Especially, the choice of data structures are affected. A data structure can store a lot of data and thus spend a lot of resources on some big object but in the end the camera might only focus on a different tiny detail and as such make most of the data useless. This problems are usually addressed with adaptive data structures.
    \item[3. High variation of light power.] Some algorithms rely on a correlation between incident flux on a shading point and the power of a light source. This is the basis for the power-based technique compared to uniform. Also the number of photons shot from a light source can be based on the light power and thus its alleged importance. This assumption is often useful, but can quickly cause problems when a strong light source is occluded close to a small light source which illuminates important details.
    \item[4. High number of contributing light sources.] The construction and lookup times of the acceleration data structures have to scale well, so that even in the case that a shading point is illuminated by many light sources rendering can stay efficient.
    \item[5. Non-axis aligned planes.] Skewed planes usually won't alight with the boundaries of the underlying data structures unless data structures like a BSP tree are used. Especially, rigid data structures like a grid can produce artifacts when the estimator seemingly arbitrarily changes. 
    \item[6. Highly occluded areas.] Large dark areas where most parts are occluded except a few spots where just a handful of photons may arrive can cause problems when trying to build meaningful estimators. Adaptive techniques may increase their search radius which can lead to unexpected behavior. Or estimators can't be build meaningfully with too few photons available.
    \item[7. Tiny but important solid angles.] The difference between a black box and an illuminated box can be a tiny hole or crack where a lot of light may enter and then scatter within. Also a small object that gets illuminated by a very strong but distant light source, e.g. the sun. The solid angle might by so tiny, that it is numerically very unlikely that photons would ever hit the object, but it may still be the main source of brightness.
\label{li:problemcases}
\end{description} 

\subsection{Test scenes}

We designed a special scene, called \textit{Stanford-Museum}, for most of the comparisons we present in this chapter. This scene covers all aforementioned problem cases and thus gives a good impression about strengths and weaknesses of each technique. The simplicity of the scene allows the reader to clearly correlate variance with the problem cases, but ultimately does not provide a photorealistic appeal. On the other hand we present two more scenes, \textit{San Miguel} and \textit{Zero-day}, for a comparison on real scenes with all sorts of details.

\paragraph{Stanford-Museum}
\label{sec:stanfordmuseum}
\begin{figure}[ht]\centering
\input{figures/StanfordMuseum.tex}
\caption{The reference image for \textit{Stanford-Museum} sampled with PBRTs directlighting integrator. Instead of NEE every light source is queried for any shading point. Max depth is one, thus no indirect illumination is present. All phenomenons from listing~\ref{li:problemcases} and more appear in this scene. Detailed descriptions and intentions of the zoomed areas are listed in~\ref{li:stanfordmuseum}.
}
\label{fig:stanfordmuseumref}
\end{figure}

We describe the most important problem cases we had in mind when designing the Stanford-Museum scene in figure~\ref{fig:stanfordmuseumref}, which is the reference image we calculate the RMSE against. The scene contains 4058 light sources, all of whom are point light sources. We chose point light sources, because sampling a point on a area light source is not a concern of PNEE, thus reducing variance from other effects does disclose artifacts produced by our techniques more clearly. We also render the scene with a pathtracer with a max depth parameter of one, again, using the same argument, as depth introduces variance from the indirect lighting term of the pathtracer. Additionally, this makes rendering a nearly perfect reference image possible, by using PBRTs direct lighting integrator, which samples all point light sources for any intersection point. The reference image is rendered with 128 samples per pixel in 17 hours on our test machine and aside from anti-aliasing should be close to perfect. 

\begin{itemize}
    \item[(1)] The dragon is 20 times smaller than the Buddah but is very close to the camera and as such takes up almost the same screen space. Several low intensity point lights illuminate the dragon. The difference in size as well as light intensity constitute typical problem cases with LOD. Especially the illumination of the eyes is a rather extreme case, light source intensity is roughly 200.000 times less compared to the strongest light sources in the scene. Additionally, the left outline of the dragon is illuminated by various light sources from bellow in the scene, where the dragon takes up only an incredibly small solid angle and thus photons are very unlikely to hit the dragon.
    \item[(2)] The Buddah stands out of all cells and as such is illuminated by almost all visible light sources in this scene. This is particularly a problem for example with \textit{photontree}. There might be more light sources influencing a shading point, than the total number of nearest neighbors we collect. High geometric detail and blending of many light colors make the Buddah prone to fireflies.  All three Stanford models are also intended to add complexity for intersection tests to various parts of the scene. 
    \item[(3)] The box has a very tiny split from which is casts long range, very bright, very thin and high frequent light rays. Additionally, a lot of photons are stuck within the thin walled box, which highly complicates building good estimators with PNEE on the outside. Several more of this boxes with varying properties and surroundings are present in the scene. 
    \item[(4)] Similar to the Buddah this plane wall is illuminated by many light sources in the scene. Additionally, there is a matrix of point lights with varying brightness, distance and color. This variety has to be mapped to smooth transitions and high frequencies accordingly. Similar scenarios to this one do also appear in other parts of the scene, for example in front of the bunny, also with many smooth transitions, but where difficulties brought by global illumination are replaced with many local soft shadows. 
    \item[(5)] A cluster of roughly 1000 light sources. Only thin, non axis-aligned walls separates it from several distinct lightning scenarios. Very high exposure (unintentionally) breaks anti-aliasing. Most light sources influence the Buddah softly from bellow. 
    \item[(6)] Several non-axis aligned thin walls cover up a small cluster of light sources and cast thin, high frequent light rays in many directions. Especially the rays casted onto the scewed box, where photons from within the box, like mentioned in (3), do no good for our estimators, do constitute a tricky shading scenario in various manners.
    \item[(7)] Several strong light sources cast long range, smoothly fading, but sharp edged light rays. A similar scenario is present on the other side of the Buddah, where colors also blend together. The rays closely pass quite dark cells. Again, strong light sources behind a thin wall interfere with our photon collection algorithms.
    \item[(8)] A completely occluded light cluster with roughly 1000 light sources. In a real scene this might be a different room of a house for example. There are four such occluded clusters in the scene.
\label{li:stanfordmuseum}
\end{itemize}
\paragraph{San Miguel}
\label{sec:sanmiguel}
San Miguel by \textcite{Sanmiguel} is a popular scene and was on the book cover of the second edition of PBRT. It offers a highly detailed and photorealistic appeal and therefor is great for illustrative purposes and evaluating our techniques in a high detail scene. As such it also serves as a realistic stress test which includes all kinds of materials, reflections, transparencies and shapes. Many of which were intentionally spared in the design of \textit{Stanford-Museum}. We intended San Miguel to be a realistic test to make sure everything works as intended. The original scene only has one light source, a distant environment light, which indeed was the only cause for problems for PNEE. Due to the current implementation, which very well may get improved in the future, the photon count had to be set higher than it normally should have. This was the only found issue.

The scene had to be enriched with more light sources to function as a test scene for our purpose. As the original c4d files are not available anymore the light sources had to be pasted manually.\unsure{ist das unwichtig? rauslassen?} There are several closed rooms in all directions to the yard. All rooms got augmented with clusters of light sources, which may constitute some realistic illumination within the rooms, but do not affect the yard respectively the view of the rendered image. In total there are 1274 light sources in the scene, of which only 13 do actually affect the view port. This was needed to be able to render a meaningful reference image. Therefor, all light sources were removed, except the 13 significant ones, and then the scene was rendered with PBRTs spatial technique with 8192 samples per pixel in 16.5 hours.\footnote{This might give spatial a slight advantage in later MSE calculations due to the general alignment of potential variance edges. This effect should be minimal though. Creating a fully unbiased reference was not feasible.} This yielded a good reference image, while all other images are rendered with all 1274 light sources, even though we know they should actually not affect the desired image.

\paragraph{Zero-Day}

Zero-Day by \textcite{Beeple} is also a very interesting and beautiful scene that in fact provides many light sources natively. With 8916 light sources it has by far the most light sources in our tests, but interestingly still doesn't make the strongest argument for PNEE. Almost all light sources are very weak and the scene is quite open, thus occlusion does not play a significant role; therefor the weaknesses of PBRTs spatial are not revealed.

The reference image was rendered with \textit{spatial} due to the lack of an better option. With 12288 ps, the sobol sampler, a max depth of seven and a 960x420 resolution the rendering took a bit over 12 hours. The image still shows a few fireflies and comparison against \textit{spatial} itself is obviously biased. We used the sobol sampler but for the comparison use the halton sampler to reduce the correlation, so that the same random samples won't create the exact same fireflies at least.



\FloatBarrier
\subsection{Techniques}

We give an brief overview over the techniques that we identified as interesting for comparison. \textit{Uniform} and \textit{power-based} sampling are naive techniques that are natively implemented in PBRT. Additionally, PBRTs just recently released take on many light sampling is referred to as \textit{spatial}, see section~\ref{}\todo{add reference to previouswork when done}. We compare these three existing techniques with our own techniques \textit{cdfgrid}, \textit{photontree} and \textit{cdftree} introduced in Table~\ref{tb:techniques}. All three of our techniques use interpolation, as this produces the cleanest results for human observers. Nonetheless, we also list \textit{cdfgrid no int}, which uses no interpolation. The images produced by this technique usually contain artifacts, but nonetheless performs very well on the MSE plots as the overall variance is better. For later plots we spare previously clearly worse performing techniques to focus our discussion.

\subsection{Parameter Comparison}

\unsure{remove this chapter? Too much dump data, not highly interesting unless someone wants to reimplement}

%%%%%%%%%%%%%%%%%
\label{ch:ev:photontree}

%%%%%%%%%%%%%%%%%
\label{ch:ev:cdftree}

%%%%%%%%%%%%%%%%%%%%

\label{ch:ev:photonsampling}


%%%%%%%%%%%%%%%%%%%%%%%
\label{ch:ev:uniformfloor}

% which parameters did we try to configure. Which parameters turned out to be good and will be set as fixed?

\section{Equal time comparisons}
\label{sec:etc}
In this section we compare and discuss our results for various techniques. We show two kinds of graphs: a $\sigma$ versus time in seconds graph and a $\sigma$ versus number of pixel samples (ps) graph; all in logarithmic scales. The time-plots make sense intuitively. The ps-plots are interesting because our test scenes for the most part do not contain fancy techniques or features that significantly increase rendering times per pixel (as e.g. \textit{photontree} does). In real world renderers though these techniques may very well exist and as such the proportional increase of rendering time per pixel sample introduced by PNEE might be significantly lowered; which in turn would make the ps-plot more significant. Additionally, it may be a better indication of how efficient a technique is disregarding the potentially sub-optimal implementation.

All data for the results showed and discussed here are available at \url{https://github.com/AndiMiko/PNEE-data}. The data contains the source for the graphs, calculated MSE and RMSE, all result images in .exr format and equally named .txt files which contain a copy of the original .pbrt configuration file and some additional rendering run logs \& analytics.

\subsection{Stanford-Museum}

\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
      \centering
        \adjincludegraphics[width=1\textwidth, trim={{.06\width} {.08\height} {.371\width} {.15\height}},clip]{figures/plots/SM_Main_t.pdf}
        \caption{}
        \label{fig:smmain_t}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \adjincludegraphics[width=1\textwidth, trim={{.06\width} {.08\height} {.371\width} {.15\height}},clip]{figures/plots/SM_Main_ps.pdf}
        \caption{}
        \label{fig:smmain_ps}
    \end{subfigure}
    \caption{Stanford-Museum time-$\sigma$ and ps-$\sigma$ log-log plots.}
    \label{fig:smmain}
\end{figure}


Considering that the plots in figure~\ref{fig:smmain} are logarithmic on both scales we see vast differences between the techniques; we discuss the most interesting phenomena here. Table~\ref{tb:sm_settings} shows the used settings. The time-plot~\ref{fig:smmain_t} includes the preprocessing time. For this reason PNEE techniques start at a later point, usually around 100 seconds. Setting may be adjusted for specific desired rendering times, e.g. to better compete in the sub 100 second range; in our case we decided to have the same render settings for all pixel sample times, as we mainly aim for the most interesting comparisons in the middle section of the plots. Log-log plots should usually be a straight line---we explain the observable curvature in~\ref{fig:smmain_t} with the preprocessing time. The graph is quickly catching up what it has lost due to the preprocess. The ps-plot~\ref{fig:smmain_ps} confirms this: as preprocessing time doesn't play a role the curvature in the beginning is gone. In the end we see the opposite effect. As the image is already very well converged the very last errors are probably not very well estimated by PNEE anymore. For this reason the PNEE techniques seem to slowly loose the very large advantage they previously had and gradually converge against what is only solvable with brute force. Slight imperfections in the reference image may also be the cause to this.\footnote{We have the comfort of being able to assume to have a nearly perfect reference image in a rather complex scene, as discussed in~\ref{sec:stanfordmuseum}. For the other scenes we do not have this possibility and for this reason we are not even considering this high number of samples per pixels in the upcoming plots.} We also speculate that it might be possible to straighten the PNEE plots by proportionally scale the preprocessing complexity with the desired pixel samples. The advantage should therefor not fall off at later stages. Anyhow, note, that the illustrated pixel sample numbers are already far above usual and the plots are still very far away from actually converging together. The curvature begins slightly at around 1024 samples per pixel, while data at above 4096 samples per pixel is mostly only interesting for scientific purposes. This is also reflected by the anomalies at around 16K pixel samples. Surprisingly, PNEE techniques are not affected, whereas \textit{uniform}, \textit{power} and \textit{spatial} are. While we are not certain, our explanation is that the halton sampler used by PBRT ran out of random numbers and starts looping there. We found slight correlative variance edges/lines that indicated this, especially in the completely broken out \textit{uniform} 32K ps image.

Focusing on the actual graphs we see a very clear winner: \textit{cdfgrid (no int)}. The naive version performs slightly better than its interpolated counterpart. The ps-plot reveals that this advantage is mostly due to the additional processing cost of the interpolation, as graphs are almost glued together there. Variance edges are nicely smoothed but overall variance increases. What weights stronger should likely be scene dependant. Even though by the numbers \textit{cdfgrid no int} may be regarded as best performing in presented plots we are confident that a human observer would choose \textit{cdfgrid} as the clear winner, refer to figure~\ref{fig:intComparison}.

The \textit{spatial} graph moves very inconsistently and interestingly, at first glance, the slope in the middle area might suggest that it should surpass all techniques at a later stage. A deeper analysis reveals that the inverted effect that we previously discussed about the curvature appears to be responsible. The technique creates several strong artifacts which give it a negative headstart, while it is correcting its own mistakes the slope is heavily favored. Mentally connecting the 16 ps and 8K ps dots in the ps- and time-plot might give a good indication of how efficient the technique would be if it would actually work in these scenarios (good but still worse than PNEE). The very same effect appears to cause the bump in the 64 ps point of the \textit{cdftree} technique. This highlights problems with the consistency of both techniques, nonetheless, these problems may rarely appear in naturally modeled scenes, as such the theoretical performance can be regarded as quite good, with both techniques occupying the midfield. 

As expected, \textit{uniform} and \textit{power} are performing significantly worse. The only arguments going for them are that they have no settings and the rendered images have a very even \enquote{undone-look}, as there are no artifacts or any differences in variance by design. Other than that they are clearly not suited for such scenes, even tough with roughly thousand light sources we are far away from imaginable extremes.

\textit{Photontree} was a surprise on the other hand. While early tests already indicated that the workload within the integrator is clearly too high the final results on the time-graph are quite striking. We want to further highlight the ps-graph. While results look much better there it is still highly surprising that it merely competes with the naive techniques. We expected clearly better results on the ps-graph, but apparently there are too many artifacts as seen in figure~\ref{fig:discontinuityEdges}\footnote{The artifacts are much more subtle in the end results.}. Visually comparing the 4K ps image of \textit{photontree} and \textit{uniform} was even more irritating\todo{add figure to appendix?}. We had to double-check whether the MSE calculation was done properly, since the \textit{photontree} image looked so much better except for a few unpleasant artifacts. Indeed it appears that the square in the MSE calculation really kicks in here, the artifacts are heavily punished in our plots. In conclusion, on the ps-plot we would easily favor the \textit{photontree} images, but the time-plot clearly deprives \textit{photontree} of any viability.

Overall the log-log plots are already indicating the vast differences, but visually comparing even more so highlights the gaps. The results range from completely unusable to excellent\todo{add figure of all techniques?}. Comparing for example the 128 ps \textit{cdfgrid} image rendered in 159 seconds with the 16K ps \textit{uniform} image rendered in 14551 seconds and similar MSE the speedup lies at roughly 90x. \textit{Cdfgrid} does not only dominate the plots, but also its ease of use and visual comparison satisfy all demands.

\begin{center}

\begin{tabular*}{\textwidth}{@{}l @{\extracolsep{\fill}} llll@{}}\toprule
 & cdfgrid no int & cdfgrid & cdftree & photontree~~~ \\ \midrule

Photon count & 96 mil & 96 mil & 50 mil & 96 mil\\
Cdf count & --- & --- & 150.000 & --- \\
Max grid cells & 96 & 96 & --- & ---  \\
Uniform floor & 0.1 & 0.1 & 0.1 & 0.1  \\
Collection Method & --- & --- & NN & NN \\
Interpolation & none & trilinear & AKR & AKR \\
NN-count & --- & --- & 16 & 192 \\
Photon threshold & --- & --- & 20 & --- \\

\bottomrule
\end{tabular*}
\captionof{table}{Render settings for Stanford-Museum for the plots in figure~\ref{fig:smmain}.}
\label{tb:sm_settings}
\end{center}







\subsection{San Miguel}
Going into the San Miguel analysis we spared \textit{photontree} because of its bad results and \textit{cdfgrid no int} because results are very similar and all valuable insights were already made in the Stanford-Museum analysis. Checking the plots in figure~\ref{fig:sanmain} we again see the previously discussed phenomena like the curved graphs of the PNEE techniques. \textit{Spatial} is very straight now, as there is no specific scenario present that create create artifacts. Overall the plots look cleaner due to the absence of the high variety of problem cases like in Stanford-Museum. \textit{Power} and \textit{cdfgrid} wiggle slightly for an unknown reason, but the effect is small enough to be potentially caused by randomness\unsure{Is this true? I don't have a better explanation. Better not to mention or what to say? Better word than randomness?} or the bias of the reference image. Used parameters are quite similar to Table~\ref{tb:sm_settings} and won't be listed again for this reason, but can be found in the Github repository.

\textit{Cdfgrid} does dominate this comparison as well. A visual comparison can be found on the cover of this work\unsure{oder soll ichs hier noch mal rein machen?}. \textit{Uniform} and \textit{power} swapped their ranking with a quite considerable gap. This illustrates that the efficiency of certain techniques is scene dependent. 



\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
      \centering
        \adjincludegraphics[width=1\textwidth, trim={{.06\width} {.08\height} {.371\width} {.15\height}},clip]{figures/plots/SanMiguel_main_t.pdf}
        \caption{}
        \label{fig:sanmain_t}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \adjincludegraphics[width=1\textwidth, trim={{.06\width} {.08\height} {.371\width} {.15\height}},clip]{figures/plots/SanMiguel_main_ps.pdf}
        \caption{}
        \label{fig:sanmain_ps}
    \end{subfigure}
    \caption{San Miguel time-$\sigma$ and ps-$\sigma$ log-log plots.}
    \label{fig:sanmain}
\end{figure}

\subsection{Zero-Day}

\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
      \centering
        \adjincludegraphics[width=1\textwidth, trim={{.06\width} {.08\height} {.371\width} {.15\height}},clip]{figures/plots/SM_Main_t.pdf}
        \caption{}
        \label{fig:zd_t}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \adjincludegraphics[width=1\textwidth, trim={{.06\width} {.08\height} {.371\width} {.15\height}},clip]{figures/plots/SM_Main_ps.pdf}
        \caption{}
        \label{fig:zd_ps}
    \end{subfigure}
    \caption{Zero-Day time-$\sigma$ and ps-$\sigma$ log-log plots.}
    \label{fig:zd}
\end{figure}

\section{Memory comparison}

We initially expected \textit{cdfgrid} to be memory-bound and as such the tradeoff between memory and time would become a key decision factor when choosing a rigid versus an adaptive data structure. As it turns out the memory consumption was not critical at all, with usage ranging between 1-4\todo{check closely again} GB for all techniques. We expected that with \textit{cdfgrid} detail could be increased until it would be bound by memory, but in fact image quality started dropping long before those boundaries were reached. This is due to the fact that cells get smaller throughout the whole scene and start destabilizing the technique. Too many small cells leave much more room for outliers which receive too few photons, or are placed in a very unfortunate position. While interpolation mitigates this problems for reasonably (still small) sized cells, at a certain degree artifacts start to sprout. \textit{Cdftree} has similar problems even amplified.  Additionally, as the evaluation in section~\ref{sec:etc} has shown \textit{cdfgrid} was able to dominate without even needing to push memory boundaries. Other techniques proved to be less consistent in image quality and speed and as such memory hasn't become the decisive factor. 

\begin{center}
\begin{tabular*}{\textwidth}{@{}l @{\extracolsep{\fill}} llll@{}}\toprule
Uniform/Power & Spatial & Cdfgrid & Photontree & Cdftree \\ \midrule
3.2 GB & 4.6 GB & 5 GB & 2,4 GB & 3,4 GB \\
\bottomrule
\end{tabular*}
\captionof{table}{Total peak memory consumption during rendering Stanford-Museum with settings as in Table~\ref{tb:sm_settings}.}
\label{tb:memory}
\end{center}

\section{Conclusions}

\begin{figure}
    \centering
      \adjincludegraphics[width=.7\textwidth, trim={{.06\width} {.08\height} {.371\width} {.15\height}},clip]{figures/plots/SM_Main_t.pdf}
    \caption{Stanford Museum comparison}
    \label{fig:smmain_t1}
\end{figure}

\begin{figure}
    \centering
    \adjincludegraphics[width=.7\textwidth, trim={{.06\width} {.08\height} {.371\width} {.15\height}},clip]{figures/plots/SM_Main_ps.pdf}
    \caption{Stanford Museum comparison}
    \label{fig:smmain_ps1}
\end{figure}
